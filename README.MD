# llama-3.2-pretrain-ops

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![License](https://img.shields.io/badge/license-MIT-green)

## ğŸ¯ Káº¿t Quáº£ Huáº¥n Luyá»‡n

![Káº¿t quáº£ huáº¥n luyá»‡n mÃ´ hÃ¬nh Llama 3.2](./ket_que_train.png)

## ğŸš€ Tá»•ng Quan

Dá»± Ã¡n nÃ y cung cáº¥p má»™t triá»ƒn khai chuyÃªn nghiá»‡p, mÃ´-Ä‘un hÃ³a vÃ  hiá»‡u nÄƒng cao cho viá»‡c huáº¥n luyá»‡n trÆ°á»›c (pretraining) cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ theo phong cÃ¡ch **Llama 3.2 1B**. ÄÆ°á»£c thiáº¿t káº¿ theo cÃ¡c tiÃªu chuáº©n ká»¹ thuáº­t cá»§a "Big Tech", dá»± Ã¡n bao gá»“m quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u máº¡nh máº½, kiáº¿n trÃºc mÃ´ hÃ¬nh hiá»‡u quáº£ vÃ  vÃ²ng láº·p huáº¥n luyá»‡n sáºµn sÃ ng cho mÃ´i trÆ°á»ng production.

Há»‡ thá»‘ng bao gá»“m cÃ¡c kháº£ nÄƒng lá»c dá»¯ liá»‡u nÃ¢ng cao nhÆ° **MinHash deduplication** (loáº¡i bá» trÃ¹ng láº·p) vÃ  **Aho-Corasick sensitive content filtering** (lá»c ná»™i dung nháº¡y cáº£m), Ä‘áº£m báº£o dá»¯ liá»‡u huáº¥n luyá»‡n cháº¥t lÆ°á»£ng cao tá»« cÃ¡c nguá»“n nhÆ° OpenWebText.

## âœ¨ TÃ­nh NÄƒng

- **Kiáº¿n TrÃºc MÃ´-Ä‘un**: PhÃ¢n tÃ¡ch rÃµ rÃ ng cÃ¡c thÃ nh pháº§n (Configs, Data, Models, Training).
- **Kiáº¿n TrÃºc Llama 3.2**:
  - Rotary Positional Embeddings (RoPE)
  - SwiGLU Activation
  - RMSNorm (Root Mean Square Normalization)
  - Grouped Query Attention (GQA)
- **Quy TrÃ¬nh Dá»¯ Liá»‡u NÃ¢ng Cao**:
  - **Deduplication**: Sá»­ dá»¥ng MinHash LSH Ä‘á»ƒ phÃ¡t hiá»‡n vÄƒn báº£n gáº§n giá»‘ng nhau.
  - **Filtering**: Thuáº­t toÃ¡n Aho-Corasick Ä‘á»ƒ cháº·n tá»« khÃ³a nhanh chÃ³ng.
  - **Streaming**: Sá»­ dá»¥ng IterableDatasets Ä‘á»ƒ xá»­ lÃ½ kho ngá»¯ liá»‡u quy mÃ´ lá»›n mÃ  khÃ´ng gáº·p váº¥n Ä‘á» vá» bá»™ nhá»›.
- **Huáº¥n Luyá»‡n Production**:
  - Mixed Precision Training (AMP)
  - Gradient Accumulation
  - TÃ­ch há»£p WandB & Tensorboard
  - Quáº£n lÃ½ Checkpoint & Early Stopping

## ğŸ“‚ Cáº¥u TrÃºc Dá»± Ãn

```
llama_pretrain/
â”œâ”€â”€ configs/                # Quáº£n lÃ½ cáº¥u hÃ¬nh
â”‚   â””â”€â”€ training_config.py  # CÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n
â”œâ”€â”€ data/                   # Quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ dataset.py          # Triá»ƒn khai PyTorch Dataset
â”‚   â”œâ”€â”€ deduplication.py    # Logic MinHash LSH
â”‚   â”œâ”€â”€ filtering.py        # Logic lá»c ná»™i dung
â”‚   â””â”€â”€ processor.py        # Xá»­ lÃ½ vÄƒn báº£n thÃ´
â”œâ”€â”€ models/                 # Äá»‹nh nghÄ©a kiáº¿n trÃºc mÃ´ hÃ¬nh
â”‚   â”œâ”€â”€ args.py             # Dataclasses cáº¥u hÃ¬nh mÃ´ hÃ¬nh
â”‚   â”œâ”€â”€ layers.py           # CÃ¡c khá»‘i Transformer (Attention, MLP)
â”‚   â”œâ”€â”€ modeling_llama.py   # Class mÃ´ hÃ¬nh chÃ­nh
â”‚   â””â”€â”€ utils.py            # Tiá»‡n Ã­ch toÃ¡n há»c (RoPE, v.v.)
â”œâ”€â”€ training/               # Äiá»u phá»‘i huáº¥n luyá»‡n
â”‚   â””â”€â”€ trainer.py          # VÃ²ng láº·p huáº¥n luyá»‡n vÃ  quáº£n lÃ½ tráº¡ng thÃ¡i
â”œâ”€â”€ utils/                  # CÃ¡c tiá»‡n Ã­ch chung
â”œâ”€â”€ main.py                 # Äiá»ƒm khá»Ÿi cháº¡y chÃ­nh (Entry point)
â”œâ”€â”€ requirements.txt        # CÃ¡c thÆ° viá»‡n phá»¥ thuá»™c
â””â”€â”€ README.md               # TÃ i liá»‡u hÆ°á»›ng dáº«n
```

## ğŸ› ï¸ CÃ i Äáº·t

1.  **Clone repository:**
    ```bash
    git clone https://github.com/yourusername/llama_pretrain.git
    cd llama_pretrain
    ```

2.  **Táº¡o mÃ´i trÆ°á»ng áº£o (khuyÃªn dÃ¹ng):**
    ```bash
    python -m venv venv
    # TrÃªn Windows:
    venv\Scripts\activate
    # TrÃªn Linux/Mac:
    source venv/bin/activate
    ```

3.  **CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n phá»¥ thuá»™c:**
    ```bash
    pip install -r requirements.txt
    ```

## ğŸƒ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### Cháº¡y Huáº¥n Luyá»‡n (Training)

Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº±ng cÃ¡ch cháº¡y file `main.py`. Script há»— trá»£ cÃ¡c tham sá»‘ dÃ²ng lá»‡nh Ä‘á»ƒ báº¡n dá»… dÃ ng tÃ¹y chá»‰nh quÃ¡ trÃ¬nh cháº¡y mÃ  khÃ´ng cáº§n sá»­a code.

**CÃº phÃ¡p cÆ¡ báº£n:**

```bash
python main.py --data_path <Ä‘Æ°á»ng_dáº«n_Ä‘áº¿n_file_dá»¯_liá»‡u> [cÃ¡c_tham_sá»‘_khÃ¡c]
```

**CÃ¡c tham sá»‘ quan trá»ng:**

| Tham sá»‘ | Kiá»ƒu | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
| :--- | :--- | :--- | :--- |
| `--data_path` | `str` | **Báº¯t buá»™c** | ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i hoáº·c tÆ°Æ¡ng Ä‘á»‘i Ä‘áº¿n file dá»¯ liá»‡u huáº¥n luyá»‡n (Ä‘á»‹nh dáº¡ng .txt). |
| `--output_dir` | `str` | `./checkpoints` | ThÆ° má»¥c Ä‘á»ƒ lÆ°u cÃ¡c checkpoint cá»§a mÃ´ hÃ¬nh. |
| `--batch_size` | `int` | `4` | KÃ­ch thÆ°á»›c batch trÃªn má»—i thiáº¿t bá»‹ (GPU/CPU). |
| `--epochs` | `int` | `3` | Tá»•ng sá»‘ epoch huáº¥n luyá»‡n. |
| `--lr` | `float` | `3e-4` | Tá»‘c Ä‘á»™ há»c (Learning rate). |
| `--max_seq_len` | `int` | `1024` | Äá»™ dÃ i chuá»—i token tá»‘i Ä‘a Ä‘áº§u vÃ o. |
| `--wandb` | `flag` | `False` | ThÃªm cá» nÃ y náº¿u muá»‘n báº­t logging lÃªn Weights & Biases. |

### VÃ­ Dá»¥ Cá»¥ Thá»ƒ

**1. Cháº¡y thá»­ nghiá»‡m nhanh vá»›i dá»¯ liá»‡u máº«u:**

```bash
python main.py --data_path ./data/sample_data.txt --epochs 1 --batch_size 2
```

**2. Cháº¡y huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§ vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh:**

```bash
python main.py \
    --data_path "E:/datasets/openwebtext_train.txt" \
    --output_dir "./models/llama_v1" \
    --batch_size 8 \
    --epochs 5 \
    --lr 1e-4 \
    --max_seq_len 2048 \
    --wandb
```

**3. Sá»­ dá»¥ng Makefile (Tiá»‡n lá»£i):**

Náº¿u báº¡n Ä‘Ã£ cáº¥u hÃ¬nh sáºµn, báº¡n cÃ³ thá»ƒ dÃ¹ng lá»‡nh make (lÆ°u Ã½ cáº§n sá»­a Makefile náº¿u muá»‘n truyá»n tham sá»‘ Ä‘á»™ng):

```bash
make train
```

## ğŸ“Š GiÃ¡m SÃ¡t (Monitoring)

Tiáº¿n trÃ¬nh huáº¥n luyá»‡n cÃ³ thá»ƒ Ä‘Æ°á»£c theo dÃµi qua:
- **Console Output**: Hiá»ƒn thá»‹ loss, learning rate vÃ  tá»‘c Ä‘á»™ xá»­ lÃ½ theo thá»i gian thá»±c.
- **WandB**: Náº¿u báº­t cá» `--wandb`, cÃ¡c biá»ƒu Ä‘á»“ chi tiáº¿t sáº½ Ä‘Æ°á»£c Ä‘áº©y lÃªn dashboard cá»§a Weights & Biases.
- **Tensorboard**: Máº·c Ä‘á»‹nh script cÅ©ng há»— trá»£ Tensorboard, log Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `output_dir/tensorboard`.

## ğŸ¤ ÄÃ³ng GÃ³p

Má»i Ä‘Ã³ng gÃ³p Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh! Vui lÃ²ng gá»­i Pull Request.

1.  Fork dá»± Ã¡n
2.  Táº¡o feature branch (`git checkout -b feature/TinhNangMoi`)
3.  Commit thay Ä‘á»•i (`git commit -m 'ThÃªm tÃ­nh nÄƒng má»›i'`)
4.  Push lÃªn branch (`git push origin feature/TinhNangMoi`)
5.  Má»Ÿ Pull Request

## ğŸ“„ Giáº¥y PhÃ©p

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c cáº¥p phÃ©p theo giáº¥y phÃ©p MIT - xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.
